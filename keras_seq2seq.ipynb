{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkoS85_7_t5x"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "Hmc9QFQRWGne",
    "outputId": "ef520e9c-c074-4c3f-8a53-4b003f55db8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.32.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.5)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (0.39)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.5)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install jieba\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "kt6kyyMaWQbH",
    "outputId": "c170c118-2d1e-41ef-808d-f807d8a719a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-01 12:19:41--  http://www.manythings.org/anki/cmn-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:30::6818:6cc4, ...\n",
      "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 545887 (533K) [application/zip]\n",
      "Saving to: ‘sample_data/cmn-eng.zip’\n",
      "\n",
      "\r",
      "sample_data/cmn-eng   0%[                    ]       0  --.-KB/s               \r",
      "sample_data/cmn-eng   9%[>                   ]  50.46K   244KB/s               \r",
      "sample_data/cmn-eng  41%[=======>            ] 218.70K   529KB/s               \r",
      "sample_data/cmn-eng 100%[===================>] 533.09K  1021KB/s    in 0.5s    \n",
      "\n",
      "2019-01-01 12:19:42 (1021 KB/s) - ‘sample_data/cmn-eng.zip’ saved [545887/545887]\n",
      "\n",
      "Archive:  sample_data/cmn-eng.zip\n",
      "  inflating: cmn.txt                 \n",
      "  inflating: _about.txt              \n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "!wget -O sample_data/cmn-eng.zip http://www.manythings.org/anki/cmn-eng.zip\n",
    "!unzip sample_data/cmn-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "-U5EGZy9wTTa",
    "outputId": "9a8c76e3-d2e4-418c-c704-8707f40f543f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-01 14:15:59--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.zh.300.vec.gz\n",
      "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 54.231.235.45\n",
      "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|54.231.235.45|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1358817100 (1.3G) [binary/octet-stream]\n",
      "Saving to: ‘cc.zh.300.vec.gz’\n",
      "\n",
      "cc.zh.300.vec.gz    100%[===================>]   1.26G  22.1MB/s    in 61s     \n",
      "\n",
      "2019-01-01 14:17:00 (21.4 MB/s) - ‘cc.zh.300.vec.gz’ saved [1358817100/1358817100]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.zh.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "bSTOoydXzfqp",
    "outputId": "fe4a84d4-366b-40f9-c548-235878f32646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000 300\n",
      "， 0.0080 0.0336 0.5720 -0.1105 -0.0200 0.0195 -0.0129 -0.0258 0.0249 -0.0010 0.0182 -0.0074 -0.0288 -0.0116 -0.0355 0.0482 0.0411 0.0008 0.0261 -0.0387 0.0104 -0.0035 0.0715 -0.0001 -0.0397 -0.0353 0.0546 -0.0462 -0.0439 0.0621 -0.0276 0.0151 -0.0528 -0.0016 0.0074 -0.0312 0.0032 -0.0843 -0.0698 -0.0309 0.0178 0.0081 -0.0203 0.0104 -0.1739 -0.0081 -0.0952 -0.0118 0.0211 0.0214 -0.0324 0.0085 -0.0114 -0.4785 -0.0216 0.0469 -0.0026 0.0152 -0.0438 0.0554 -0.0441 -0.0256 -0.0084 -0.0121 -0.0372 -0.0109 0.0704 0.0208 -0.0457 -0.0252 -0.0115 0.0414 0.0078 0.0451 0.0373 0.0149 0.0373 0.0944 0.0333 -0.0020 -0.0143 -0.0006 -0.0178 -0.0050 0.0223 0.0022 0.0922 0.0001 0.0413 0.0115 -0.0239 -0.0202 -0.0142 0.0135 0.0253 -0.0556 -0.0220 -0.0096 0.0073 -0.0090 -0.3952 0.0566 0.0040 -0.0037 0.0147 0.0125 0.0362 0.0277 -0.0226 -0.0104 -0.0305 0.0660 -0.0232 -0.0053 -0.0105 -0.0029 0.0037 0.0066 -0.0339 -0.0758 0.0686 0.0273 -0.0338 -0.0198 -0.0310 -0.0315 0.0456 0.0422 -0.0045 -0.0045 0.1647 0.0284 -0.0198 -0.0238 0.0197 -0.0003 0.0740 0.0135 0.0243 -0.1015 0.0212 0.0513 -0.0684 -0.0116 -0.0024 -0.0356 -0.0080 -0.0313 -0.0351 -0.0167 -0.0361 0.0589 -0.0683 0.0073 -0.0447 -0.0078 0.0186 0.0159 -0.0034 -0.0568 0.0110 -0.0200 -0.0099 -0.0180 -0.0392 -0.0321 -0.0016 0.0097 0.0425 0.0377 -0.0780 0.0008 0.0027 0.1349 -0.0381 0.0314 -0.0674 -0.0167 -0.0152 0.0187 0.0119 -0.3675 0.0160 0.0079 -0.0137 -0.1024 -0.0176 0.0084 0.0156 0.3850 0.0075 0.2133 -0.0301 0.0102 -0.0216 -0.0117 0.0126 0.0301 0.0425 0.0355 0.0285 0.0262 -0.0272 0.0001 0.0022 -0.0259 -0.0001 -0.0016 0.0475 -0.0252 -0.0223 -0.0654 0.0414 0.0226 0.0164 0.0366 -0.0470 -0.0330 -0.0516 0.0377 -0.0017 -0.0178 -0.0429 -0.0089 -0.0955 -0.0232 0.0325 0.0490 0.0310 0.1112 -0.0315 0.3998 -0.0017 -0.0222 -0.0107 -0.0078 -0.0120 0.0222 -0.0090 -0.0087 -0.0123 -0.0300 0.0197 0.0201 0.0025 -0.0147 0.0209 -0.0021 0.0020 -0.0712 -0.0126 0.0264 -0.0066 0.0175 -0.0047 0.0184 -0.0017 0.0491 0.0511 -0.0158 0.0110 -0.0119 0.0155 0.0321 0.0286 -0.0123 0.0267 0.0008 -0.0129 0.0001 0.0392 0.0286 -0.0021 -0.0174 0.0176 0.1274 0.0834 0.0703 0.0453 0.0135 -0.2507 0.1832 -0.0441 -0.1480 -0.0212 0.0112 -0.0088 0.0209 -0.0378 0.0397 -0.0048 -0.0002 0.0708 0.0012 -0.0368 0.0169 0.0230 -0.0106 0.0009 -0.0085\n",
      "的 -0.0119 0.0510 0.4397 -0.0427 -0.0501 -0.0258 -0.0060 -0.0676 -0.0776 -0.0291 -0.0105 0.0245 -0.0393 0.0320 -0.0111 0.0186 0.0137 -0.0045 0.2983 -0.0247 -0.0003 0.0286 0.0269 0.0334 -0.0227 -0.0668 0.0930 -0.0471 -0.0254 0.0596 -0.0322 0.0152 -0.0308 0.0084 -0.0034 -0.0029 -0.0161 -0.1025 -0.0579 -0.0437 -0.0017 -0.0314 0.0179 0.0142 0.1478 -0.0082 -0.1265 -0.0046 0.0350 -0.0176 -0.0124 0.0075 0.0230 -0.6317 -0.0289 0.0105 0.0293 -0.0419 -0.0058 -0.0202 -0.0088 -0.0270 -0.0554 -0.0101 0.0137 -0.0477 0.0154 -0.0382 -0.0198 -0.0303 0.0369 0.0874 0.0130 0.0425 0.0129 -0.0001 0.0563 0.0614 0.0180 -0.0405 0.0328 0.0474 0.0250 -0.0264 0.0157 0.0180 0.0622 -0.0356 -0.0241 0.1892 0.1045 -0.0495 -0.0412 0.0275 0.0046 -0.0428 0.0024 0.0219 0.0415 0.0485 -0.1524 -0.0243 0.0085 0.1101 0.3530 0.0051 0.0080 -0.0002 -0.0149 0.0185 -0.0073 0.0189 0.0958 0.0417 0.0388 0.0202 0.0102 -0.0454 -0.0498 -0.0403 0.0148 -0.0357 0.0140 0.0091 0.0545 -0.0478 0.0205 0.0774 0.0026 -0.0120 -0.0301 -0.0041 -0.0463 -0.0266 0.0299 0.0257 0.0894 0.0017 -0.0158 -0.0552 -0.0171 0.0623 -0.1284 0.0056 0.0538 -0.0623 -0.0613 0.0291 -0.0277 -0.0236 0.0322 0.0487 -0.0307 0.1110 -0.0007 0.0186 0.0179 -0.0059 0.0130 -0.1050 -0.0264 -0.0017 -0.0045 -0.0193 -0.0158 -0.0243 0.0274 0.0416 0.0574 0.0366 -0.0806 0.0017 -0.0389 0.1026 0.0277 0.0564 -0.0127 0.0151 -0.0135 0.0086 -0.0163 -0.5472 -0.0076 0.0150 0.0055 -0.0433 0.0204 0.0115 0.0488 0.4071 0.0123 0.0297 -0.0454 0.0173 0.0265 -0.0335 -0.0192 0.0202 0.0887 -0.0712 0.0294 0.0100 -0.0083 0.0108 -0.0389 -0.0030 -0.0069 0.0208 -0.0501 -0.0516 -0.0027 0.0197 -0.0104 0.0017 -0.0208 0.0252 -0.0292 -0.0288 0.0093 0.0444 -0.0075 0.0358 -0.0079 -0.0192 -0.1218 -0.0021 0.0146 0.0546 0.0238 0.1482 0.0041 0.4287 -0.0070 -0.0303 0.0021 0.0000 0.0400 0.0402 -0.0185 -0.0063 -0.0419 -0.0480 -0.0081 -0.0350 -0.0243 -0.0570 0.0781 -0.0431 -0.0979 0.1655 -0.0600 0.0208 0.0018 -0.0313 0.0135 0.0118 0.0515 0.0513 0.0514 -0.0253 -0.0243 -0.0362 -0.0119 -0.0013 -0.0026 0.0344 0.0158 -0.0056 -0.0356 0.0353 0.0308 0.0304 -0.0105 -0.0250 -0.0342 0.0540 -0.0098 0.0464 0.0643 0.0014 -0.1998 0.1056 -0.0114 -0.1235 0.0061 0.0043 -0.0403 -0.0063 0.1638 0.0070 0.0012 -0.0619 0.0235 -0.0119 0.2803 -0.0090 -0.0071 -0.0585 0.0369 -0.0270\n",
      "。 0.0093 0.0210 0.7688 -0.0348 -0.0471 0.0540 -0.0309 -0.0107 -0.0753 0.0024 0.0311 0.0160 -0.0219 0.0165 -0.0441 0.0748 0.0413 -0.0002 -0.0220 -0.0255 -0.0140 0.0312 0.0416 -0.0051 0.0218 0.0137 0.0516 -0.0358 -0.0378 0.0395 -0.0461 -0.0141 -0.0443 -0.0072 0.0598 -0.0480 0.0020 -0.0898 -0.0783 0.0001 -0.0040 -0.0214 0.0385 0.0343 -0.0680 0.0057 -0.0921 0.0110 0.0421 0.0140 -0.0278 0.0560 -0.0113 -0.5788 -0.0016 0.0400 0.0140 0.0508 -0.0330 0.0114 0.0049 -0.0486 -0.0520 0.0282 -0.0248 -0.0168 -0.0021 -0.0126 -0.0370 -0.0050 0.0354 0.0690 -0.0138 0.0357 0.0482 -0.0068 0.0003 0.0532 0.0386 -0.0332 0.0069 0.0111 0.0177 0.0177 -0.0183 -0.0125 0.0002 -0.0223 -0.0295 0.5587 0.0747 -0.0302 -0.0090 0.0075 0.0332 -0.0555 -0.0374 0.0101 0.0342 0.0332 -0.4357 0.0163 -0.0225 0.0695 0.2925 -0.0146 0.0368 0.0388 -0.0112 -0.0023 -0.0488 0.0339 -0.0038 0.0442 0.0288 0.0067 -0.0013 -0.0114 -0.0271 -0.0843 0.0979 0.0036 -0.0095 0.0268 0.0003 -0.0410 0.0802 0.0268 -0.0223 0.0052 0.1238 0.0201 0.0028 -0.0128 0.0209 0.0316 0.0399 0.0270 0.0537 -0.1150 0.0110 0.0411 -0.0237 0.0090 -0.0302 -0.0335 -0.0028 -0.1421 -0.0414 -0.0282 -0.0554 0.0730 -0.0341 0.0291 -0.0432 0.0150 0.0328 0.0120 0.0122 0.0101 0.0134 -0.0489 -0.0069 -0.0062 -0.0258 -0.0691 0.0105 0.0227 0.0202 0.0171 -0.0259 0.0018 -0.0024 0.1080 -0.0154 0.0384 -0.0444 -0.0195 -0.0370 0.0099 -0.0326 -0.3826 0.0042 -0.0557 0.0140 -0.0150 0.0053 0.0506 0.0029 0.1761 0.0153 0.3617 -0.0200 0.0304 0.0036 0.0120 -0.0071 0.0252 0.0324 0.0609 0.0282 0.0004 -0.0311 0.0136 0.0293 -0.0230 -0.0359 0.0151 0.0220 0.0094 0.0311 0.0125 0.0063 0.0727 0.0180 0.0549 -0.0389 -0.0186 -0.2065 -0.0059 -0.0079 -0.0978 -0.0761 0.0164 -0.0830 0.0101 0.0183 0.0002 0.0282 0.0870 0.0028 0.3800 -0.0002 -0.0895 0.0113 -0.0283 0.0033 0.0203 -0.0009 -0.0090 -0.0364 0.0038 -0.0190 0.0077 0.0215 -0.0258 -0.0342 0.0461 0.0199 -0.1385 0.0223 0.0391 -0.0002 0.0016 -0.0035 0.0097 0.0139 0.0502 0.0270 -0.0316 0.0704 0.0029 0.0303 0.0062 0.0980 -0.0388 0.0090 -0.0005 -0.0382 -0.0212 0.0621 0.0266 -0.0274 -0.0233 -0.0212 0.1521 0.0285 0.0499 0.0478 0.0232 -0.2323 0.1945 -0.0252 -0.0623 -0.0098 0.0008 -0.0560 0.0136 0.0154 0.0693 -0.0141 0.0074 0.0764 -0.0135 -0.3037 -0.0191 0.0020 -0.0336 0.0148 -0.0056\n",
      "</s> -0.0112 -0.0688 1.0076 0.0070 -0.0300 -0.0195 -0.0107 0.0716 -0.0900 0.0181 0.0214 0.0380 0.0356 0.0117 -0.0568 0.0403 0.0268 0.0039 -0.2992 0.0242 0.0003 0.0099 -0.0918 -0.0000 0.1598 0.0231 -0.0195 0.0648 0.0150 -0.0275 -0.0099 -0.0099 0.0317 -0.0374 0.0412 0.0189 -0.0258 0.1369 -0.0202 0.0061 0.0063 0.0127 -0.0188 0.0135 -0.2256 0.0020 0.1060 0.0372 0.0078 0.0208 -0.0124 0.1026 0.0398 -0.4826 0.0416 -0.0329 -0.0320 0.1092 0.0157 -0.0180 0.0499 -0.0307 0.0021 0.0024 0.0078 -0.0311 -0.0429 -0.0796 0.0282 0.0234 0.0540 -0.0827 -0.0030 -0.0433 -0.1525 0.0100 -0.0229 -0.0142 -0.0381 0.0120 -0.0056 -0.0271 0.0144 -0.0256 -0.0559 -0.0280 -0.0691 -0.0395 -0.0273 0.6128 0.0464 -0.0097 0.0378 0.0379 0.0238 0.0910 -0.0168 0.0309 0.0151 0.0225 -0.4296 -0.0212 -0.0355 0.0284 0.6087 -0.0205 -0.0431 -0.0330 0.0174 -0.0003 -0.0515 -0.0194 0.0553 0.0833 0.0077 0.0049 -0.0194 -0.0440 -0.0589 -0.0004 0.0909 -0.0628 0.0398 0.0585 0.0039 -0.0244 0.0341 0.0481 0.0025 0.0359 -0.0981 -0.0222 0.0157 0.0310 0.0297 0.0005 -0.0306 0.0983 0.0760 -0.0540 -0.0280 -0.0578 0.0221 0.0160 -0.1489 0.0284 0.0561 0.0893 -0.0060 -0.0122 0.0006 -0.0035 0.1154 0.0020 0.0113 0.0194 0.0249 0.0326 0.0479 -0.0276 0.0213 0.0409 0.0299 0.0328 0.0555 -0.0292 0.0131 0.0283 -0.0174 -0.0253 0.0120 -0.0364 -0.0257 -0.0572 0.0096 -0.0199 -0.0371 0.0305 -0.0029 0.0141 -0.0309 -0.3223 -0.0390 -0.0713 0.0350 0.1768 -0.0224 0.0421 -0.0363 -0.1605 -0.0191 0.3061 0.0393 0.0477 0.0368 0.0244 -0.0265 -0.0335 -0.0063 -0.0136 -0.0138 0.0190 -0.0768 0.0080 0.0041 0.0101 0.0103 -0.0197 0.0142 0.0159 0.0371 0.0015 0.0004 0.0292 0.0141 0.0264 0.0487 0.0247 -0.1128 -0.0515 0.0374 -0.0233 0.0234 0.0614 -0.0035 0.0583 -0.0306 -0.0421 -0.0184 -0.0550 0.0111 0.2772 -0.0298 -0.1294 0.0298 -0.0840 -0.0022 -0.0328 -0.0155 0.0241 -0.0143 0.0727 -0.0289 -0.0475 0.0387 0.0072 -0.1012 0.0214 0.0222 -0.0438 0.0243 -0.0368 -0.0023 0.0211 0.0435 -0.0361 0.0563 -0.0301 0.0237 -0.0391 0.1075 0.0447 0.0288 -0.0338 0.0480 -0.0165 -0.0242 0.0541 -0.0158 -0.0533 0.0321 -0.0227 -0.0357 0.0093 -0.0459 0.1033 -0.0176 0.0013 -0.0527 0.0292 0.3792 -0.2120 0.0488 0.1958 0.0413 -0.0282 0.0196 0.0217 0.0101 0.0196 0.0026 0.0513 -0.0286 -0.0097 0.5390 -0.0155 -0.0403 0.0147 0.0184 -0.0247\n",
      "、 0.0658 0.0075 0.5058 0.0821 -0.0212 0.0265 -0.0398 0.0324 -0.0415 -0.0111 0.0660 0.0186 0.0024 0.0711 0.0618 0.0627 -0.0479 -0.0433 -0.0423 0.0555 -0.0230 -0.0057 0.0825 -0.1146 -0.0410 0.0904 0.0497 -0.0098 0.0239 0.0554 -0.0380 -0.0627 -0.0226 -0.0135 0.0248 0.0628 0.0454 -0.1094 -0.0548 -0.0098 -0.0140 0.0674 0.0493 -0.0598 -0.0657 0.0472 -0.0244 -0.0653 0.0076 0.0155 -0.0149 0.0941 0.0521 -0.7917 0.0749 0.0755 -0.1613 -0.0404 -0.0486 -0.0030 0.0529 -0.0063 0.0050 0.1140 -0.1793 0.0322 -0.1041 -0.0608 0.0533 -0.0192 -0.0435 -0.0291 -0.0171 -0.1339 -0.0636 0.0006 -0.0169 0.0850 0.0064 0.0889 -0.1337 0.0170 0.0099 0.1623 0.0015 -0.0390 -0.0573 0.1697 -0.0326 -0.0287 -0.1404 -0.0282 0.0041 -0.0293 -0.0592 -0.0391 -0.0041 -0.0754 0.0300 0.0458 -0.0872 0.1749 0.0377 0.0943 -0.0497 0.0360 -0.1089 0.0518 -0.0490 -0.1344 0.0458 -0.0042 -0.0618 -0.0319 0.0375 0.0555 0.0446 0.0549 0.0868 -0.0122 -0.2750 -0.0196 0.0513 0.0491 0.0552 0.0421 0.0829 0.0701 -0.0381 0.0858 0.1725 -0.0282 -0.1139 -0.0116 -0.0624 0.0351 -0.1751 -0.0427 -0.0156 0.0862 0.0597 0.0093 0.1503 -0.0049 0.0369 -0.0443 0.0590 -0.1104 -0.0056 0.0047 0.0279 -0.0283 -0.1458 -0.1051 0.0394 -0.0281 -0.0578 -0.0679 -0.0212 -0.1036 -0.1762 0.0305 -0.0440 0.0420 0.0828 -0.0163 -0.0453 0.0112 -0.0621 -0.0243 0.1208 0.0250 -0.0123 0.0685 0.1569 -0.0962 -0.0978 0.0106 0.0202 -0.1426 -0.0061 -0.7137 -0.0103 -0.0270 0.0124 -0.0358 -0.0081 -0.0027 -0.0869 0.3081 -0.0286 -0.0119 0.0036 -0.0081 -0.0024 0.0156 -0.0277 -0.0398 -0.0800 0.0846 0.0281 0.0309 -0.0465 -0.0072 -0.0974 -0.0154 -0.0245 -0.0031 0.0183 0.0831 0.0359 0.0071 0.0358 -0.0430 -0.0409 0.0522 -0.0010 0.0230 0.0094 0.0477 -0.0285 0.0715 0.0111 0.0665 0.2245 0.0963 -0.0156 -0.1189 0.0325 0.0906 -0.1088 0.3549 0.0616 -0.0410 0.0289 -0.0005 0.0371 -0.0676 0.0132 -0.0801 -0.0259 0.0035 -0.0034 -0.0510 -0.0546 0.0219 -0.0081 0.0067 -0.0169 -0.0217 0.0092 0.0277 0.0049 -0.0438 -0.0571 -0.0092 0.0573 -0.1016 0.0419 -0.0222 0.0046 -0.0052 -0.0173 0.0125 0.0032 -0.0110 0.0552 0.0786 0.0073 0.0455 -0.1291 -0.0402 -0.0300 -0.1210 -0.0400 0.0615 0.0608 -0.0646 -0.0009 -0.1353 -0.2334 0.0807 0.0412 -0.1165 0.0306 -0.0769 -0.0301 0.0160 0.1011 0.0549 0.0046 0.0188 0.0137 0.1212 -0.0209 -0.0515 -0.0104 0.0354 0.1606 0.0252\n",
      "是 -0.0058 0.0187 0.5163 -0.0498 -0.0657 0.0018 -0.0819 -0.1081 0.0015 -0.0077 -0.0031 0.0129 -0.0757 0.0314 -0.1106 0.0271 0.0475 -0.0421 0.1421 -0.0188 0.0084 0.0436 0.0050 0.0828 -0.0468 0.0155 0.1209 -0.0873 -0.0146 0.0743 -0.0242 0.0312 0.0546 -0.0347 0.0508 -0.0391 0.0305 -0.0965 -0.0557 -0.1038 0.0361 -0.0225 0.0269 0.0238 -0.1553 -0.0299 -0.1167 -0.0113 -0.0090 0.0364 -0.0194 -0.0156 -0.0420 -0.5698 0.0052 -0.0256 0.0030 0.0347 -0.0347 -0.0924 0.0712 -0.0833 -0.0141 -0.0434 -0.0180 -0.0351 0.0867 -0.0057 -0.0040 -0.0410 -0.0033 0.0613 -0.0023 0.0729 0.0552 -0.0264 0.0678 -0.0297 0.0246 -0.0531 0.0647 0.0811 0.0326 -0.1236 0.0331 0.0524 -0.1028 -0.0288 -0.0019 -0.3939 0.1425 -0.0129 -0.0409 0.0735 0.0542 -0.0386 -0.0247 0.0023 0.0444 -0.0081 -0.2708 -0.0764 0.0322 0.0695 0.0246 0.0460 -0.0008 -0.0143 0.0559 -0.0369 -0.0093 -0.0086 0.0914 0.0103 0.0747 0.0401 -0.0250 -0.0670 -0.1911 -0.0446 -0.1768 0.0995 -0.0250 0.0036 0.0527 -0.0254 -0.0810 0.0816 0.0533 0.0424 0.1522 -0.0317 -0.0883 0.0268 -0.0754 0.0629 0.0562 0.0449 -0.0484 -0.0586 -0.0601 0.1661 -0.2884 0.0273 -0.1002 -0.0848 0.0050 -0.0278 0.0036 -0.0759 0.0741 0.0387 0.0090 0.0403 -0.0311 0.0301 0.0160 -0.0256 0.0246 0.0054 -0.0211 0.0190 -0.0300 -0.0469 -0.0013 -0.0054 -0.0303 0.0458 0.0077 0.0352 -0.1077 0.0230 -0.0989 0.1899 -0.0384 0.0512 0.0458 0.0150 0.0259 -0.0081 -0.0254 -0.5028 -0.0357 0.0711 -0.0285 0.0078 0.0699 -0.0154 0.0404 0.3918 0.0096 0.0580 -0.0240 0.1180 -0.0046 -0.0952 0.0268 0.0175 0.1170 -0.0040 0.0555 0.0391 -0.0301 0.0352 -0.0689 -0.0128 -0.0602 0.0146 -0.0013 -0.0017 0.0765 -0.0844 -0.0054 0.0225 0.0260 0.0075 -0.0029 -0.0728 -0.0103 0.0796 -0.0277 0.0804 -0.0506 -0.0143 -0.2193 -0.0512 0.0429 0.0675 0.0015 0.1367 -0.0418 0.4088 -0.0962 0.0290 0.0675 0.0571 0.0119 0.0103 -0.0226 -0.0042 -0.0458 -0.0970 0.0701 -0.0228 -0.0076 -0.0782 0.0232 -0.0417 -0.0475 0.0281 -0.0523 -0.0105 -0.0113 -0.0207 0.0315 0.0349 0.0295 0.0471 0.0894 -0.0572 -0.0234 -0.0566 -0.0825 -0.0004 0.0659 0.0270 0.0140 0.0006 -0.0781 -0.0736 0.0366 0.0402 0.0188 0.0208 -0.0722 0.0593 -0.0280 0.0979 0.0552 -0.0404 -0.2497 0.1906 -0.0154 -0.1057 0.0296 -0.0377 -0.0229 -0.0319 -0.1423 0.0192 0.0452 -0.1217 0.0490 -0.0318 -0.1454 -0.0198 -0.0274 -0.0382 0.0177 -0.0239\n",
      "一 -0.0605 0.0828 0.4169 -0.0256 -0.0545 0.0549 0.0142 -0.0211 0.0000 0.0202 -0.0118 -0.0546 -0.0181 0.0453 -0.0408 -0.0090 -0.0161 -0.0653 -0.3033 -0.0060 -0.0145 0.0284 0.0420 0.0074 -0.1146 -0.1269 0.1179 -0.0050 -0.0510 0.0281 -0.0343 -0.0120 0.0297 0.0734 0.1022 -0.0679 0.0286 -0.1410 -0.0578 0.0159 -0.0484 0.0022 0.0036 -0.0518 -0.2017 0.0295 -0.0233 0.0136 0.0860 0.0347 0.0248 0.0314 -0.0358 -0.5774 -0.0177 0.0227 0.0362 0.0094 -0.0613 -0.0244 0.0325 -0.1273 -0.0386 0.0078 -0.0417 -0.0537 0.0104 0.0329 -0.0140 -0.0870 0.0544 0.0436 0.0091 0.0352 0.0464 0.0278 0.0175 -0.0432 0.0063 -0.0390 -0.0946 0.0171 0.0277 0.0066 0.0285 0.0219 0.0160 -0.0930 -0.0400 0.1028 0.3100 -0.0606 -0.0805 0.0121 0.0564 -0.0475 0.0365 0.0274 0.0016 -0.0473 -0.0630 -0.0031 -0.0098 -0.0624 0.6480 0.0042 0.0139 -0.0114 0.0435 0.0347 -0.0293 0.0180 0.0210 0.0780 0.0978 0.0323 -0.0049 -0.0280 -0.1558 -0.0441 0.0724 0.0129 0.0013 0.0022 -0.0022 -0.0081 -0.0013 -0.1023 -0.0037 -0.0258 0.1546 -0.0155 -0.1286 0.0150 -0.0881 0.0009 0.0642 0.0935 -0.0008 -0.0732 0.0294 0.0774 -0.4455 -0.0091 -0.0133 -0.0740 -0.0299 0.0723 -0.0497 -0.0511 0.0927 0.0370 -0.0355 0.0574 -0.0385 -0.0159 0.0447 -0.0004 0.0136 -0.0384 -0.0075 0.0063 -0.0342 0.0173 -0.0340 -0.0339 0.0435 0.0555 0.0631 0.0422 -0.0683 -0.0315 -0.0320 0.0724 0.0076 0.0486 0.0274 0.0390 -0.0357 0.0282 0.0056 0.0546 -0.0659 0.0204 0.0059 -0.0133 0.0615 0.0714 0.0607 0.4042 0.0287 -0.3525 -0.0618 0.0399 0.0531 -0.0843 -0.0400 -0.0175 -0.0481 -0.1554 0.0274 0.0255 -0.1434 0.0463 -0.0020 -0.0558 -0.0801 -0.0174 -0.0215 -0.0384 0.0591 0.0810 0.0079 0.0612 -0.0314 0.0501 0.0779 -0.0593 0.1919 0.0234 0.0251 -0.0006 -0.0049 0.0080 -0.1716 -0.0344 0.0119 0.0526 -0.0073 0.1367 0.0146 0.4579 0.0021 0.1001 0.0402 0.1816 -0.0474 0.0236 -0.0299 0.0219 -0.0056 -0.0807 0.0271 -0.0329 -0.0053 -0.0889 0.0779 -0.0198 -0.0289 0.0528 -0.0384 0.0163 0.0034 0.0339 -0.0108 0.0163 0.0623 -0.0118 0.0594 -0.0713 -0.0768 -0.0181 0.0050 -0.0024 0.0448 0.0433 -0.0197 -0.0044 -0.0010 0.0313 -0.0145 0.0178 -0.0140 -0.0328 -0.0386 0.1216 0.0797 0.0283 -0.0087 0.0523 -0.2668 0.1744 0.0535 -0.0256 0.0169 0.0367 -0.0018 0.0242 -0.0133 0.0293 0.0280 -0.0078 0.0435 0.0356 0.3525 -0.0398 0.0221 -0.0237 -0.0102 -0.0164\n",
      "在 0.0637 0.0220 0.4530 -0.0489 -0.0472 0.0745 0.0454 -0.0560 -0.0528 -0.0168 0.0110 0.0752 -0.0035 -0.0648 -0.0895 -0.0360 0.0138 0.0252 -0.0047 0.0095 -0.0525 0.0105 0.0574 0.0004 -0.0701 -0.1308 0.0884 -0.0672 -0.0459 0.0260 -0.1194 0.0211 -0.0214 -0.0454 0.0336 0.0231 -0.0068 -0.1407 -0.0688 0.0317 -0.0337 -0.0078 0.0032 0.0053 -0.0063 -0.0497 -0.2602 -0.0101 -0.0642 -0.0097 0.0349 -0.0183 0.0353 -0.5978 -0.0264 0.0792 -0.0291 -0.1306 -0.0117 0.0820 0.0233 0.0266 -0.0398 0.0286 0.0523 -0.0119 0.0449 -0.0028 -0.0350 0.0779 0.0593 0.0834 0.0067 0.0677 -0.0359 0.0140 0.0341 0.0865 0.0546 0.0139 -0.0113 0.0152 0.0652 0.0059 0.0066 0.0351 0.1249 -0.0168 0.0748 -0.0148 0.0353 -0.0292 -0.0759 -0.0355 0.0567 0.0196 -0.0269 -0.0312 0.0469 0.0243 -0.3382 -0.0617 0.0364 -0.2491 -0.2647 0.0310 0.0475 0.0174 -0.0210 -0.0028 -0.0659 0.0420 0.1114 -0.0114 0.0303 0.0153 0.0483 -0.0012 -0.1329 -0.0692 -0.1833 -0.0021 -0.0229 0.0025 0.0490 -0.0298 0.0116 0.0805 -0.0050 0.0405 0.1961 0.1427 0.0267 -0.0609 0.0140 0.0150 0.0988 -0.0192 -0.0327 -0.0291 0.0071 0.0258 0.0268 0.0086 -0.0239 -0.0484 -0.0700 -0.1274 -0.0558 -0.0721 0.0283 0.1255 -0.0292 0.0687 -0.0406 -0.0335 -0.0279 0.0307 0.0171 -0.1187 0.0088 0.0197 0.0402 -0.0203 0.0311 -0.0884 0.0315 -0.0027 0.0616 0.0020 -0.1432 -0.0245 0.0171 0.0774 -0.0587 0.0526 -0.0878 -0.0087 -0.0175 0.0171 -0.0470 -0.5309 0.0522 -0.0567 0.0432 0.0352 0.0365 -0.0103 0.0674 0.3785 -0.0108 0.0520 -0.0359 -0.0033 0.0694 -0.0653 0.0416 0.0647 -0.0067 -0.0686 -0.0257 -0.0082 0.0194 -0.0464 0.0113 -0.0704 0.0264 0.0375 0.0727 -0.0873 -0.0207 0.0373 -0.0140 0.1230 -0.0268 0.0096 -0.0992 -0.0229 0.0714 0.0590 -0.0651 0.0561 -0.0334 -0.0250 -0.1119 -0.0001 -0.0388 0.0576 -0.0242 0.1509 0.0098 0.4034 0.0128 -0.0183 0.0250 0.0244 -0.0477 0.0292 -0.0046 0.0491 -0.0028 -0.0330 0.0045 0.0032 0.0138 -0.0073 -0.0178 -0.0149 0.0517 -0.0661 -0.0175 0.0680 0.0641 -0.0665 0.0128 0.0318 0.0144 0.0802 0.0428 -0.0030 0.0217 -0.0167 0.0117 -0.0377 -0.1127 0.0756 0.0576 0.0306 -0.0359 0.0016 0.0511 0.0432 -0.0260 -0.0356 -0.0295 0.0894 0.0544 0.0036 0.0273 0.0694 -0.2571 0.1347 -0.0061 -0.2020 -0.0249 0.0454 -0.0777 -0.0586 0.0884 0.0033 0.0023 -0.0392 0.0362 0.0379 -0.2699 0.0416 0.0350 -0.0997 0.0176 -0.0563\n",
      "： 0.0762 -0.0601 0.4058 0.0772 0.0251 0.0558 -0.0049 0.0919 -0.0200 0.0683 -0.0310 -0.0558 -0.0427 -0.0433 0.0622 0.0959 -0.0046 -0.0547 0.2134 0.0667 0.0662 -0.0033 -0.0631 0.0466 -0.1303 -0.0419 0.1621 0.0702 -0.0303 0.0824 0.0285 -0.0858 0.0509 0.0786 0.1003 0.0852 -0.0312 -0.0619 0.0303 -0.0351 -0.0159 0.0192 0.0020 0.0008 -0.1962 -0.0560 0.0251 -0.0424 -0.1232 0.0329 0.0078 -0.0101 -0.0083 -0.5394 0.0749 0.0205 -0.0253 0.0002 -0.0542 -0.2464 -0.1251 -0.0200 0.0137 -0.0057 -0.0626 -0.0897 0.1961 0.0216 -0.0369 -0.0776 0.0111 -0.0480 -0.0399 -0.0123 0.1004 0.0378 0.0547 -0.0022 0.0385 -0.0547 0.1308 0.0945 0.0318 -0.0299 0.0114 0.0023 -0.1449 0.0399 0.0129 -0.0615 -0.0142 -0.0516 -0.1045 0.0416 0.0923 -0.0328 0.1280 0.0147 -0.0561 0.0029 -0.4152 -0.0678 0.0390 -0.1290 -0.0583 -0.0412 -0.0024 -0.0405 0.0326 -0.0544 -0.0090 0.0129 0.0648 0.1440 -0.0162 0.0534 0.0283 -0.0580 0.0564 0.0684 -0.0570 -0.0503 0.0117 0.0353 0.0292 0.0389 -0.0294 0.0002 -0.0527 0.0468 0.1002 -0.0785 -0.0426 0.0583 -0.1347 -0.0217 -0.0494 0.0083 -0.0203 -0.0249 0.0042 0.0301 -0.0367 -0.0071 -0.1704 -0.0405 0.1114 0.0467 0.0713 -0.0288 -0.0686 0.0792 0.0959 -0.0361 -0.0241 -0.0528 -0.0847 0.0160 -0.0437 -0.0837 -0.0451 0.0478 -0.0771 0.0622 0.0133 -0.1305 -0.0560 0.0694 -0.0670 0.0560 0.1032 -0.0208 0.0542 0.0456 -0.0267 -0.0141 -0.0908 0.0743 -0.0252 -0.0946 -0.1312 -0.4335 0.0280 0.0352 -0.0411 0.0966 0.0198 0.0570 -0.0362 0.1669 0.0316 0.0239 -0.0026 -0.0568 -0.0006 0.0444 0.0019 0.0156 0.0373 -0.1943 -0.0411 0.0250 0.1287 0.0282 -0.0603 0.1050 -0.0502 0.0056 -0.0789 0.0590 0.1174 0.0097 0.0705 -0.0522 0.0672 -0.0587 0.0204 -0.0322 0.1226 -0.0090 0.0321 -0.0857 -0.0358 -0.0303 0.1935 0.0245 0.0682 0.0132 -0.0511 -0.0055 -0.0560 0.3767 -0.0008 -0.0557 0.0530 -0.0168 0.0445 -0.0213 0.0279 -0.0444 -0.0698 -0.0090 -0.0546 0.0058 -0.0513 -0.0339 0.0050 0.0261 0.0359 -0.2250 0.0733 0.0070 -0.0361 -0.0033 0.0243 -0.0080 -0.0625 -0.1629 -0.0406 0.0708 -0.0890 -0.0185 -0.0319 -0.0363 -0.0532 0.0310 0.0042 0.0056 -0.0151 -0.1529 -0.0623 -0.0551 -0.0713 -0.0428 -0.0200 -0.0117 -0.0656 -0.0296 0.0017 -0.0547 -0.1759 -0.1906 0.0551 0.1716 -0.0015 -0.0384 0.0529 0.1346 -0.0675 -0.0228 0.0207 -0.0424 -0.0097 -0.0364 -0.1778 -0.0567 -0.0236 0.0341 0.0010 0.0488\n"
     ]
    }
   ],
   "source": [
    "!head cc.zh.300.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48bFqQdsxd0O"
   },
   "outputs": [],
   "source": [
    "# Load Chinese Pre-trained Word2vec\n",
    "# It takes some time to load \n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = \"/content/cc.zh.300.vec\"\n",
    "\n",
    "cn_word2vec = KeyedVectors.load_word2vec_format(glove_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-mvIODQ9y29g",
    "outputId": "893f76d2-db16-4a28-85ae-10bd00cef90f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# Testing chinese Word Embedding\n",
    "test_cn_word = cn_word2vec['蛇']\n",
    "print(test_cn_word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "bytgTJHpgwcd",
    "outputId": "48f5767e-55ad-4c70-ac05-8138e717c474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-01 13:39:07--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2019-01-01 13:39:07--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  20.9MB/s    in 42s     \n",
      "\n",
      "2019-01-01 13:39:50 (19.4 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get pretrained word Embeddings(from fasttext)\n",
    "# English: https://s3-us-west-1.amazonaws.com/fasttext-vectors/cc.en.300.bin.gz\n",
    "# Chinese: https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.zh.300.bin.gz\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USwD9GtKmozi"
   },
   "outputs": [],
   "source": [
    "# Load English Pre-trained Word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = \"/content/glove.6B.200d.txt\"\n",
    "tmp_file = get_tmpfile(\"./test_word2vec.txt\")\n",
    "\n",
    "# call glove2word2vec script\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "en_word2vec = KeyedVectors.load_word2vec_format(tmp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jpzWK64juJMX",
    "outputId": "c89cf48d-969f-4b89-c38e-2fa438cd3841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Testing English Word Embedding\n",
    "import numpy as np\n",
    "test_en_word = en_word2vec['going']\n",
    "print(test_en_word.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2210
    },
    "colab_type": "code",
    "id": "_urWPvfiWEz2",
    "outputId": "d72354b5-f865-4027-cdaf-293cf40a1a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Only Load first 10000\n",
      "Turn Tokens into index\n",
      "initialization of input\n",
      "shape of encoder_input: (10000, 14), decoder_input: (10000, 15), output: (10000, 15, 7002)\n",
      "Fill values into np array\n",
      "Start Trianing!\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 5.0987 - val_loss: 4.8651\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 4.2051 - val_loss: 4.7890\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 4.0057 - val_loss: 4.6279\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 3.8113 - val_loss: 4.5735\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 3.6465 - val_loss: 4.5534\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 3.4892 - val_loss: 4.3948\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 3.3387 - val_loss: 4.3354\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 3.1890 - val_loss: 4.2683\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 3.0493 - val_loss: 4.2539\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 2.9101 - val_loss: 4.2074\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 2.7832 - val_loss: 4.1239\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 2.6531 - val_loss: 4.0962\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 2.5391 - val_loss: 4.0754\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 2.4168 - val_loss: 4.0298\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 2.2951 - val_loss: 4.0130\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 2.1900 - val_loss: 3.9983\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 2.0770 - val_loss: 3.9653\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.9659 - val_loss: 3.9127\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.8578 - val_loss: 3.9045\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.7583 - val_loss: 3.9144\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.6542 - val_loss: 3.8944\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.5621 - val_loss: 3.8666\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.4591 - val_loss: 3.8497\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.3757 - val_loss: 3.8511\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.2819 - val_loss: 3.8315\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.1960 - val_loss: 3.8343\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.1147 - val_loss: 3.8204\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 1.0387 - val_loss: 3.8273\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.9624 - val_loss: 3.8256\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.8957 - val_loss: 3.8115\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.8280 - val_loss: 3.8960\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.7670 - val_loss: 3.8360\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.7136 - val_loss: 3.8693\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.6486 - val_loss: 3.8496\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.6095 - val_loss: 3.8376\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5472 - val_loss: 3.8404\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5151 - val_loss: 3.8606\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4591 - val_loss: 3.8528\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4260 - val_loss: 3.8645\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3884 - val_loss: 3.9094\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3553 - val_loss: 3.8787\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3134 - val_loss: 3.9252\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2967 - val_loss: 3.9361\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2587 - val_loss: 3.9689\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2356 - val_loss: 3.9619\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2171 - val_loss: 3.9624\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1923 - val_loss: 4.0116\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1733 - val_loss: 4.0171\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1539 - val_loss: 4.0350\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1398 - val_loss: 4.0199\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1250 - val_loss: 4.0795\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1119 - val_loss: 4.0730\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1017 - val_loss: 4.0798\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0945 - val_loss: 4.1371\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0795 - val_loss: 4.1304\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0737 - val_loss: 4.2161\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0708 - val_loss: 4.2170\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0636 - val_loss: 4.1801\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0591 - val_loss: 4.1822\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0522 - val_loss: 4.2450\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import jieba\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Read Data\n",
    "data_path = \"./cmn.txt\"\n",
    "data = []\n",
    "if os.path.exists(data_path):\n",
    "    with open(data_path) as f:\n",
    "        raw = f.read()\n",
    "        lines = list(raw.split(\"\\n\"))\n",
    "        lines = lines[0:-1]\n",
    "        for line in lines:\n",
    "            if type(line) == type(\"\"):\n",
    "                per_pair = line.split(\"\\t\")\n",
    "                data.append((per_pair[0], per_pair[1]))\n",
    "else:\n",
    "    print(\"Fail to read data!\")\n",
    "print(\"Only Load first 10000\")\n",
    "data = data[:10000]\n",
    "\n",
    "# Tokenizer\n",
    "cn_lookup = {\"<PAD>\":0, \"<START>\":1, \"<END>\":2,\"<UNK>\":3}\n",
    "en_lookup = {\"<PAD>\":0, \"<START>\":1, \"<END>\":2,\"<UNK>\":3}\n",
    "cn_idx = 4\n",
    "en_idx = 4\n",
    "\n",
    "tokens = []\n",
    "for en, cn in data:\n",
    "    cn_token_list = [\"<START>\"]\n",
    "    en_token_list = [\"<START>\"]\n",
    "    cn_token_list.extend(list(jieba.cut(cn)))\n",
    "    cn_token_list.extend([\"<END>\"])\n",
    "    en_token_list.extend(list(nltk.word_tokenize(en)))\n",
    "    en_token_list.extend([\"<END>\"])\n",
    "    for cn_token in cn_token_list:\n",
    "        if cn_token not in cn_lookup:\n",
    "            cn_lookup[cn_token] = cn_idx\n",
    "            cn_idx += 1\n",
    "    for en_token in en_token_list:\n",
    "        if en_token not in en_lookup:\n",
    "            en_lookup[en_token] = en_idx\n",
    "            en_idx += 1\n",
    "    \n",
    "    tokens.append((en_token_list, cn_token_list))\n",
    "\n",
    "# Training Parameters\n",
    "paras = {\n",
    "        \"MAX_EN_LEN\" : max([len(token[0]) for token in tokens]),\n",
    "        \"MAX_CN_LEN\" : max([len(token[1]) for token in tokens]),\n",
    "        \"EN_VOCAB_SIZE\": len(en_lookup),\n",
    "        \"CN_VOCAB_SIZE\": len(cn_lookup),\n",
    "        \"NUM_OF_SAMPLES\" : len(tokens)\n",
    "}\n",
    "\n",
    "# Turn tokens into index\n",
    "print(\"Turn Tokens into index\")\n",
    "_tokens = []\n",
    "for en_token_list, cn_token_list in tokens:\n",
    "    en_idx_tokens = []\n",
    "    for token in en_token_list:\n",
    "        en_idx_tokens.append(en_lookup[token])\n",
    "    cn_idx_tokens = []\n",
    "    for token in cn_token_list:\n",
    "        cn_idx_tokens.append(cn_lookup[token])\n",
    "    _tokens.append((en_idx_tokens, cn_idx_tokens))\n",
    "\n",
    "# Initialization of input\n",
    "print(\"initialization of input\")\n",
    "encoder_input_data = np.zeros(\n",
    "        (paras[\"NUM_OF_SAMPLES\"], paras[\"MAX_EN_LEN\"]),\n",
    "        dtype='float32'\n",
    "        )\n",
    "decoder_input_data = np.zeros(\n",
    "        (paras[\"NUM_OF_SAMPLES\"], paras[\"MAX_CN_LEN\"]),\n",
    "        dtype='float32'\n",
    "        )\n",
    "decoder_target_data = np.zeros(\n",
    "        (paras[\"NUM_OF_SAMPLES\"], paras[\"MAX_CN_LEN\"], paras[\"CN_VOCAB_SIZE\"] ),\n",
    "        dtype='float32'\n",
    "        )\n",
    "print(\"shape of encoder_input: {}, decoder_input: {}, output: {}\".format(\n",
    "    encoder_input_data.shape,\n",
    "    decoder_input_data.shape,\n",
    "    decoder_target_data.shape\n",
    "    ))\n",
    "\n",
    "# Fill in values into np array\n",
    "print(\"Fill values into np array\")\n",
    "for idx, (en_idx_tokens, cn_idx_tokens) in enumerate(_tokens):\n",
    "    for _idx, wd in enumerate(en_idx_tokens):\n",
    "        encoder_input_data[idx, _idx] = wd\n",
    "    for _idx, wd in enumerate(cn_idx_tokens):\n",
    "        decoder_input_data[idx, _idx] = wd\n",
    "        if _idx > 0:\n",
    "            decoder_target_data[idx, _idx-1, wd] = 1\n",
    "\n",
    "# Build pre-trained matrix\n",
    "EN_EMBEDDING_DIM = 200\n",
    "CN_EMBEDDING_DIM = 300\n",
    "cn_embedding_matrix = np.zeros((paras[\"CN_VOCAB_SIZE\"], CN_EMBEDDING_DIM), dtype=\"float32\")\n",
    "en_embedding_matrix = np.zeros((paras[\"EN_VOCAB_SIZE\"], EN_EMBEDDING_DIM), dtype=\"float32\")\n",
    "\n",
    "# if we found pretrained vector, then place it in the matrix\n",
    "for key in en_lookup:\n",
    "      if key in en_word2vec:\n",
    "        en_embedding_matrix[en_lookup[key]] = en_word2vec[key]\n",
    "\n",
    "for key in cn_lookup:\n",
    "      if key in cn_word2vec:\n",
    "        cn_embedding_matrix[cn_lookup[key]] = cn_word2vec[key]\n",
    "            \n",
    "# Model\n",
    "# Define\n",
    "HIDDEN_DIM = 512\n",
    "# encoder\n",
    "encoder_embed = Embedding(\n",
    "    paras[\"EN_VOCAB_SIZE\"], \n",
    "    EN_EMBEDDING_DIM, \n",
    "    weights=[en_embedding_matrix],\n",
    "    input_length=paras[\"MAX_EN_LEN\"\n",
    "], mask_zero=True)\n",
    "encoder_lstm = LSTM(HIDDEN_DIM, return_state = True)\n",
    "encoder_inputs = Input(shape = (paras[\"MAX_EN_LEN\"],))\n",
    "embed_inputs = encoder_embed(encoder_inputs)\n",
    "_, state_h, state_c = encoder_lstm(embed_inputs)\n",
    "context_state = [state_h, state_c]\n",
    "# decoder\n",
    "decoder_embed = Embedding(\n",
    "    paras[\"CN_VOCAB_SIZE\"], \n",
    "    CN_EMBEDDING_DIM, \n",
    "    weights=[cn_embedding_matrix],\n",
    "    input_length=paras[\"MAX_CN_LEN\"\n",
    "], mask_zero=True)\n",
    "decoder_lstm = LSTM(HIDDEN_DIM, return_state = True, return_sequences = True)\n",
    "decoder_dense = Dense(paras[\"CN_VOCAB_SIZE\"], activation=\"softmax\")\n",
    "decoder_inputs = Input(shape = (paras[\"MAX_CN_LEN\"], ))\n",
    "embed_inputs = decoder_embed(decoder_inputs)\n",
    "outputs, _, _ = decoder_lstm(\n",
    "    embed_inputs,\n",
    "    initial_state = context_state\n",
    "    )\n",
    "outputs = decoder_dense(outputs)\n",
    "\n",
    "# Run\n",
    "print(\"Start Trianing!\")\n",
    "train_paras = {\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 50\n",
    "    }\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs )\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "    batch_size = train_paras[\"batch_size\"],\n",
    "    epochs = train_paras[\"epochs\"],\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "model.save('./s.h5')\n",
    "\n",
    "# Define sampling models\n",
    "# encoder_model\n",
    "encoder_model = Model(encoder_inputs, context_state)\n",
    "# decoder_model\n",
    "decoder_state_input_h = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_embed_inputs = decoder_embed(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embed_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "reverse_cn_lookup = {}\n",
    "for key in cn_lookup:\n",
    "    reverse_cn_lookup[cn_lookup[key]] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhHe7EkBod05"
   },
   "outputs": [],
   "source": [
    "def translate(input_seq):\n",
    "    context_states = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, paras[\"MAX_CN_LEN\"]), dtype=\"float32\")\n",
    "    target_seq[0, 0] = cn_lookup[\"<START>\"]\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "                [target_seq] + context_states)\n",
    "        token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "        predict_token = reverse_cn_lookup[token_idx]\n",
    "        decoded_sentence += predict_token\n",
    "        if predict_token == \"<END>\" or len(decoded_sentence) > paras[\"MAX_CN_LEN\"]:\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, paras[\"MAX_CN_LEN\"]), dtype=\"float32\")\n",
    "        target_seq[0, 0] = token_idx\n",
    "        context_states = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hSt9fX7Y2ga"
   },
   "outputs": [],
   "source": [
    "# Run\n",
    "# 4077, 2122, 3336, 1463, 8954, 7164, 3491, 4495, 5097, 118\n",
    "chosen = [4077, 2122, 3336, 1463, 8954, 7164, 3491, 4495, 5097, 118 ]\n",
    "for idx in chosen:\n",
    "    fed = encoder_input_data[idx:idx+1]\n",
    "      \n",
    "    print(tokens[idx])\n",
    "    sent = translate(fed)\n",
    "    print(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQYVmjf8DoZ3"
   },
   "outputs": [],
   "source": [
    "# Save result to output.txt\n",
    "with open('output.txt', 'w') as f:\n",
    "    for idx in chosen:\n",
    "        fed = encoder_input_data[idx:idx+1]\n",
    "        input_sent = tokens[idx]\n",
    "        input_sent = input_sent[0]\n",
    "        decoded_sent = translate(fed)\n",
    "        f.write(\"Input sentence\\t\" + input_sent)\n",
    "        f.write(\"Decoded sentence\\t\" + decoded_sent)\n",
    "        f.write(\"-\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
